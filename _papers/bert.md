---
title: "BERT: Pre-training of Deep Bidirectional Transformers"
authors:
  - Jacob Devlin
  - Ming-Wei Chang
  - Kenton Lee
year: 2019
arxiv: "1810.04805"
category: nlp
tags: [pretraining, nlp, transformers, bert]
date_added: 2024-01-05
summary: |
  We introduce BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text.
---
