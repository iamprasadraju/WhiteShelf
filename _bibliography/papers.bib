---
---
@article{vaswani2017attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  title     = {Attention Is All You Need},
  journal   = {Advances in Neural Information Processing Systems},
  year      = {2017},
  volume    = {30},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.}
}

@article{brown2020language,
  author    = {Brown, Tom B. and others},
  title     = {Language Models are Few-Shot Learners},
  journal   = {Advances in Neural Information Processing Systems},
  year      = {2020},
  volume    = {33},
  doi       = {10.48550/arXiv.2005.14165},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. We test this hypothesis by training a language model on one billion slides.}
}

@article{ho2020denoising,
  author    = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  title     = {Denoising Diffusion Probabilistic Models},
  journal   = {Advances in Neural Information Processing Systems},
  year      = {2020},
  volume    = {33},
  doi       = {10.48550/arXiv.2006.11239},
  abstract  = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.}
}

@article{radford2021learning,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and others},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {International Conference on Machine Learning},
  year      = {2021},
  doi       = {10.48550/arXiv.2103.00020},
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This closed set assumption restricts the model's ability to generalize to new visual concepts.}
}

@article{devlin2019bert,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {Proceedings of NAACL-HLT},
  year      = {2019},
  doi       = {10.48550/arXiv.1810.04805},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.}
}
